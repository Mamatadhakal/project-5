{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d85a1752-2d83-4d9b-97f6-715d55534bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mamat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mamat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mamat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary libraries for data manipulation and analysis\n",
    "import pandas as pd # For dataframes and data manipulation\n",
    "import numpy as np  # For numerical operations\n",
    "import matplotlib.pyplot as plt  # For data visualization\n",
    "import seaborn as sns # For enhanced data visualization\n",
    "\n",
    "# Importing Natural Language Toolkit (NLTK) components for text processing\n",
    "from nltk.corpus import stopwords # For common stopwords\n",
    "from nltk.tokenize import word_tokenize # For splitting text into words (tokens)\n",
    "from nltk.stem import WordNetLemmatizer # For reducing words to their base/dictionary form\n",
    "from nltk.stem import PorterStemmer # For reducing words to their root form (more aggressive than lemmatization)\n",
    "import string # For string operations and punctuation characters\n",
    "\n",
    "# Importing scikit-learn components for machine learning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # For converting text to TF-IDF features\n",
    "from sklearn.model_selection import train_test_split # For splitting data into train/test sets\n",
    "from sklearn.linear_model import LogisticRegression  # For logistic regression classifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score # For model evaluation\n",
    "\n",
    "# Importing NLTK and downloading necessary datasets/models\n",
    "import nltk\n",
    "nltk.download('punkt') # Downloading the Punkt tokenizer models\n",
    "nltk.download('stopwords') # Downloading common stopwords list\n",
    "nltk.download('wordnet') #download the WordNet lexical database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6f19b22-6d90-45df-a043-6605c8dedda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/mamat/Downloads/Reviews.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58249520-8bd0-41b8-acf2-45c60fc31c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm # Import tqdm library for progress bars\n",
    "tqdm.pandas() # Enable pandas integration with tqdm progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c080fa20-5a88-4e64-a413-e181fa304481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 85204/568454 [1:23:36<32:15:27,  4.16it/s]"
     ]
    }
   ],
   "source": [
    "# Define the text preprocessing function (same as before)  \n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    tokens = [word for word in tokens if word.isalnum() and word not in stopwords.words('english')]  # Remove stopwords and non-alphanumeric\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]  # Apply stemming\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the entire 'Text' column with progress bar\n",
    "df['cleaned_text'] = df['Text'].progress_apply(preprocess_text)\n",
    "\n",
    "# Preview result\n",
    "print(df[['Text', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cf438f-ab57-46bb-b796-a5045a8308c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to convert numerical scores to sentiment categories\n",
    "def label_sentiment(score):\n",
    "    \"\"\"\n",
    "    Converts 1-5 star ratings into sentiment categories:\n",
    "    - 4-5 stars → Positive\n",
    "    - 3 stars → Neutral \n",
    "    - 1-2 stars → Negative\n",
    "    \"\"\"\n",
    "    if score >= 4:\n",
    "        return 'Positive'  # 4 or 5 star reviews are positive\n",
    "    elif score == 3:\n",
    "        return 'Neutral' # 3 star reviews are neutral\n",
    "    else:\n",
    "        return 'Negative' # 1 or 2 star reviews are negative\n",
    "\n",
    "df['Sentiment'] = df['Score'].apply(label_sentiment) # Apply the labeling function to create a new 'Sentiment' column\n",
    "\n",
    "# Check label distribution\n",
    "print(df['Sentiment'].value_counts())\n",
    "# This shows how many reviews fall into each category\n",
    "# Helps identify class imbalance issues before modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d40f30-b9b5-4fb5-ad68-9ef95dab999b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer # Import TF-IDF Vectorizer from scikit-learn\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "# Limit to top 5000 most important words/features\n",
    "# Other default but important parameters being used:\n",
    "# - lowercase=True (already handled in our preprocessing)\n",
    "# - tokenizer=word_tokenize (but we pre-tokenized)\n",
    "# - stop_words='english' (already removed in preprocessing)\n",
    "# - ngram_range=(1,1) (only single words by default)\n",
    "\n",
    "X = vectorizer.fit_transform(df['cleaned_text']) # Transform cleaned text into TF-IDF feature matrix\n",
    "# What this does:\n",
    "# 1. fit_transform() learns the vocabulary (all unique words)\n",
    "# 2. Computes inverse document frequency (IDF) weights\n",
    "# 3. Transforms text to numerical feature vectors\n",
    "# Result is a sparse matrix where:\n",
    "# - Rows = reviews\n",
    "# - Columns = word features (5000 dimensions)\n",
    "# - Values = TF-IDF scores (importance of word in document)\n",
    "\n",
    "# Target labels\n",
    "y = df['Sentiment'] # Our Positive/Neutral/Negative labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89561b3-e237-439c-8dce-9fe4ee384e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 20% of data for testing (80% for training)\n",
    "# 42=Seed for reproducible splits\n",
    "\n",
    "# Resulting variables:\n",
    "# X_train: Feature matrix for training (4000 samples if n=5000)\n",
    "# X_test: Feature matrix for testing (1000 samples)\n",
    "# y_train: Labels for training\n",
    "# y_test: Labels for testing (ground truth for evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1e2741-e119-466f-aa01-199c799a5128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# Initialize the Logistic Regression model with key parameters:\n",
    "model = LogisticRegression(max_iter=1000) # Maximum number of iterations for solver to converge\n",
    "model.fit(X_train, y_train) # Train the model on our training data\n",
    "\n",
    "# After training, the model can:\n",
    "# Make predictions on new text data\n",
    "# Show which words are most important for each sentiment\n",
    "# Estimate prediction probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86facd7e-f19b-4377-ab5c-8287a896d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "# This applies the trained model to the unseen test data\n",
    "# Returns predicted sentiment labels (Positive/Neutral/Negative)\n",
    "\n",
    "# Evaluation metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred)) #Accuracy Score (Overall correctness)\n",
    "# What this shows:\n",
    "# - Percentage of correctly classified reviews\n",
    "# - Simple overall measure but can be misleading with class imbalance\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred)) #Detailed Classification Report\n",
    "# Provides key metrics for EACH CLASS:\n",
    "# - Precision: % of correct predictions for each sentiment \n",
    "#   (e.g., when predicting Positive, how often correct)\n",
    "# - Recall: % of actual cases captured for each sentiment\n",
    "#   (e.g., what % of true Positive reviews were identified)\n",
    "# - F1-score: Harmonic mean of precision and recall\n",
    "# - Support: Number of actual occurrences in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d800ea-23d4-4886-844b-a449a51f29a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=['Positive', 'Neutral', 'Negative'])\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(6,4)) # Set figure size for readability\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Positive', 'Neutral', 'Negative'],  # x-axis labels\n",
    "            yticklabels=['Positive', 'Neutral', 'Negative']) # y-axis labels\n",
    "\n",
    "plt.title('Confusion Matrix') # Add titles\n",
    "plt.xlabel('Predicted Label') # Add labels\n",
    "plt.ylabel('True Label') # Add labels\n",
    "plt.show() #to view visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e7809a-aaac-42ca-a670-f663cdffce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Distribution\n",
    "# Plot a bar chart showing the count of each sentiment category\n",
    "df['Sentiment'].value_counts().plot(kind='bar', color=['green', 'gray', 'red'])\n",
    "# Colors are intuitively chosen: green for positive, red for negative, gray for neutral\n",
    "plt.title('Sentiment Distribution') # Add chart title and axis labels for better readability\n",
    "plt.xlabel('Sentiment') # Label for x-axis (sentiment categories)\n",
    "plt.ylabel('Count') # Label for y-axis (number of occurrences)\n",
    "plt.show() # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9f3fe3-7281-485f-b670-69ccda825ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data with sentiment labels\n",
    "df.to_csv('processed_reviews.csv', index=False)\n",
    "\n",
    "# Save predictions\n",
    "results = pd.DataFrame({'True Sentiment': y_test, 'Predicted Sentiment': y_pred})\n",
    "results.to_csv('predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
